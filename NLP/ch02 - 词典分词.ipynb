{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "addb8c6e",
   "metadata": {},
   "source": [
    "长尾效应\n",
    "\n",
    "# 1.准备词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc36dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhanlp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a52da55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dictionary():\n",
    "    \"\"\"\n",
    "    加载HanLP中的mini词库\n",
    "    :return: 一个set形式的词库\n",
    "    \"\"\"\n",
    "    IOUtil = JClass('com.hankcs.hanlp.corpus.io.IOUtil') # 根据Java类得到一个Python类\n",
    "    path = HanLP.Config.CoreDictionaryPath.replace('.txt', '.mini.txt')\n",
    "    dic = IOUtil.loadDictionary([path])\n",
    "    return set(dic.keySet())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bdd1fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85584\n",
      "贺年卡\n"
     ]
    }
   ],
   "source": [
    "dic = load_dictionary()\n",
    "print(len(dic))\n",
    "print(list(dic)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2719fd5",
   "metadata": {},
   "source": [
    "# 2.切分算法\n",
    "\n",
    "## 2.1.完全切分\n",
    "\n",
    "不是标准意义的切词，找出所有单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22ab84a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_segment(text, dic):\n",
    "    word_list = []\n",
    "    for i in range(len(text)):                  # i 从 0 到text的最后一个字的下标遍历\n",
    "        for j in range(i + 1, len(text) + 1):   # j 遍历[i + 1, len(text)]区间\n",
    "            word = text[i:j]                    # 取出连续区间[i, j]对应的字符串\n",
    "            if word in dic:                     # 如果在词典中，则认为是一个词\n",
    "                word_list.append(word)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1037e3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['商', '商品', '品', '和', '和服', '服', '服务', '务']\n"
     ]
    }
   ],
   "source": [
    "dic = load_dictionary()\n",
    "print(fully_segment('商品和服务', dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d03386",
   "metadata": {},
   "source": [
    "## 2.2.正向最长匹配\n",
    "\n",
    "优先输出更长的单词（最长匹配算法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98e34ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_segment(text, dic):\n",
    "    word_list = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        longest_word = text[i]                      # 当前扫描位置的单字\n",
    "        for j in range(i + 1, len(text) + 1):       # 所有可能的结尾\n",
    "            word = text[i:j]                        # 从当前位置到结尾的连续字符串\n",
    "            if word in dic:                         # 在词典中\n",
    "                if len(word) > len(longest_word):   # 并且更长\n",
    "                    longest_word = word             # 则更优先输出\n",
    "        word_list.append(longest_word)              # 输出最长词\n",
    "        i += len(longest_word)                      # 正向扫描\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61f8999f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['就读', '北京大学']\n",
      "['研究生', '命', '起源']\n"
     ]
    }
   ],
   "source": [
    "dic = load_dictionary()\n",
    "print(forward_segment('就读北京大学', dic))\n",
    "print(forward_segment('研究生命起源', dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f9b2a0",
   "metadata": {},
   "source": [
    "## 2.3.逆向最长匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "305bf8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_segment(text, dic):\n",
    "    word_list = []\n",
    "    i = len(text) - 1\n",
    "    while i >= 0:                                   # 扫描位置作为终点\n",
    "        longest_word = text[i]                      # 扫描位置的单字\n",
    "        for j in range(0, i):                       # 遍历[0, i]区间作为待查询词语的起点\n",
    "            word = text[j: i + 1]                   # 取出[j, i]区间作为待查询单词\n",
    "            if word in dic:\n",
    "                if len(word) > len(longest_word):   # 越长优先级越高\n",
    "                    longest_word = word\n",
    "                    break\n",
    "        word_list.insert(0, longest_word)           # 逆向扫描，所以越先查出的单词在位置上越靠后\n",
    "        i -= len(longest_word)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31e5e37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['研究', '生命', '起源']\n"
     ]
    }
   ],
   "source": [
    "dic = load_dictionary()\n",
    "print(backward_segment('研究生命起源', dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e562a9",
   "metadata": {},
   "source": [
    "## 2.4.双向最长匹配\n",
    "\n",
    "正向最长和逆向最长的取长补短\n",
    "\n",
    "逆向最长的成功次数更多\n",
    "\n",
    "流程：\n",
    "\n",
    "- 同时执行正向和逆向，若两者词数不同，返回词数更少的那个\n",
    "\n",
    "- 否则，返回两者中单字更少的那一个。当单字数也相同时，优先返回逆向最长匹配的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dc38036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_single_char(word_list: list):  # 统计单字成词的个数\n",
    "    return sum(1 for word in word_list if len(word) == 1)\n",
    "\n",
    "\n",
    "def bidirectional_segment(text, dic):\n",
    "    f = forward_segment(text, dic)\n",
    "    b = backward_segment(text, dic)\n",
    "    if len(f) < len(b):                                  # 词数更少优先级更高\n",
    "        return f\n",
    "    elif len(f) > len(b):\n",
    "        return b\n",
    "    else:\n",
    "        if count_single_char(f) < count_single_char(b):  # 单字更少优先级更高\n",
    "            return f\n",
    "        else:\n",
    "            return b                                     # 都相等时逆向匹配优先级更高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "860fad1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['研究', '生命', '起源']\n"
     ]
    }
   ],
   "source": [
    "dic = load_dictionary()\n",
    "print(bidirectional_segment('研究生命起源', dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e67831",
   "metadata": {},
   "source": [
    "## 2.5.速度测评\n",
    "\n",
    "词典分词的核心价值在速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfe17a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f0ddf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_speed(segment, text, dic):\n",
    "    start_time = time.time()\n",
    "    for i in range(pressure):\n",
    "        segment(text, dic)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('%.2f 万字/秒' % (len(text) * pressure / 10000 / elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e3e22c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "由于JPype调用开销巨大，以下速度显著慢于原生Java\n",
      "133.49 万字/秒\n",
      "120.31 万字/秒\n",
      "57.90 万字/秒\n"
     ]
    }
   ],
   "source": [
    "text = \"江西鄱阳湖干枯，中国最大淡水湖变成大草原\"\n",
    "pressure = 10000\n",
    "dic = load_dictionary()\n",
    "print('由于JPype调用开销巨大，以下速度显著慢于原生Java')\n",
    "evaluate_speed(forward_segment, text, dic)\n",
    "evaluate_speed(backward_segment, text, dic)\n",
    "evaluate_speed(bidirectional_segment, text, dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf98286",
   "metadata": {},
   "source": [
    "# 3.字典树\n",
    "\n",
    "匹配算法的瓶颈在于判断集合中是否有字符串。如果用有序集合（TreeMap），复杂度是O(logn)。\n",
    "\n",
    "使用字典树（trie树、前缀树）存储，每一条路径都可以表示一个字符串。在相同前缀的字符串上有了加速时间和节省空间。\n",
    "\n",
    "## 3.1.字典树的节点实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46d16c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, value) -> None:\n",
    "        self._children = {}\n",
    "        self._value = value\n",
    "\n",
    "    def _add_child(self, char, value, overwrite=False):\n",
    "        child = self._children.get(char)\n",
    "        if child is None:\n",
    "            child = Node(value)\n",
    "            self._children[char] = child\n",
    "        elif overwrite:\n",
    "            child._value = value\n",
    "        return child"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8914e8c0",
   "metadata": {},
   "source": [
    "## 3.2.增删改查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "246aadb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trie(Node):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(None)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return self[key] is not None\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        state = self\n",
    "        for char in key:\n",
    "            state = state._children.get(char)\n",
    "            if state is None:\n",
    "                return None\n",
    "        return state._value\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        state = self\n",
    "        for i, char in enumerate(key):\n",
    "            if i < len(key) - 1:\n",
    "                state = state._add_child(char, None, False) # 还在路上\n",
    "            else:\n",
    "                state = state._add_child(char, value, True) # 在该节点取到该值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22a34f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trie = Trie()\n",
    "# 增\n",
    "trie['自然'] = 'nature'\n",
    "trie['自然人'] = 'human'\n",
    "trie['自然语言'] = 'language'\n",
    "trie['自语'] = 'talk\tto oneself'\n",
    "trie['入门'] = 'introduction'\n",
    "assert '自然' in trie\n",
    "# 删\n",
    "trie['自然'] = None\n",
    "assert '自然' not in trie\n",
    "# 改\n",
    "trie['自然语言'] = 'human language'\n",
    "assert trie['自然语言'] == 'human language'\n",
    "# 查\n",
    "assert trie['入门'] == 'introduction'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37274ed",
   "metadata": {},
   "source": [
    "# 4.双数组字典树\n",
    "\n",
    "（Double Array Trie，DAT），状态转移复杂度为常数（BinTrie为logn），由base和check两个数组组成。\n",
    "\n",
    "双数组字典树是DFA（有穷状态机），其状态由base和check中的元素和下标表示。\n",
    "\n",
    "当状态b接受字符c转移到状态p：\n",
    "\n",
    "`p = base[b] + c`\n",
    "\n",
    "`check[p] = base[b]`\n",
    "\n",
    "## 4.1.状态转移\n",
    "\n",
    "由于Python默认的散列函数不适合字符。可以借用Java的hashCode()\n",
    "\n",
    "## 4.2.查询\n",
    "\n",
    "有了transition，对key的查询就变成至多len(key)+1次状态转移，多出来的一次针对' \\0 '。\n",
    "\n",
    "先查询key的字典序，再去取出value。\n",
    "\n",
    "具体做法：将字典序作为自然数赋予作为单词结尾的那些节点。约定当状态p满足`base[p] < 0`时，对应单词结尾，且单词的字典序为-base[p]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d10897f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleArrayTrie(object):\n",
    "    def __init__(self, dic: dict) -> None:\n",
    "        m = JClass('java.util.TreeMap')()\n",
    "        for k, v in dic.items():\n",
    "            m[k] = v\n",
    "        DoubleArrayTrie = JClass('com.hankcs.hanlp.collection.trie.DoubleArrayTrie')\n",
    "        dat = DoubleArrayTrie(m)\n",
    "        self.base = dat.getBase()\n",
    "        self.check = dat.getCheck()\n",
    "        self.value = dat.getValueArray([''])\n",
    "\n",
    "    @staticmethod\n",
    "    def char_hash(c) -> int:\n",
    "        return JClass('java.lang.Character')(c).hashCode()\n",
    "\n",
    "    def transition(self, c, b) -> int:\n",
    "        \"\"\"\n",
    "        状态转移\n",
    "        :param c: 字符\n",
    "        :param b: 初始状态\n",
    "        :return: 转移后的状态，-1表示失败\n",
    "        \"\"\"\n",
    "        p = self.base[b] + self.char_hash(c) + 1\n",
    "        if self.base[b] == self.check[p]:\n",
    "            return p\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def __getitem__(self, key: str):\n",
    "        b = 0\n",
    "        for i in range(0, len(key)):  # len(key)次状态转移\n",
    "            print(key[i])\n",
    "            p = self.transition(key[i], b)\n",
    "            print(p)\n",
    "            if p != -1:\n",
    "                b = p\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        p = self.base[b]  # 按字符'\\0'进行状态转移\n",
    "        n = self.base[p]  # 查询base\n",
    "        print(n)\n",
    "        if p == self.check[p] and n < 0:  # 状态转移成功且对应词语结尾\n",
    "            index = -n - 1  # 取得字典序\n",
    "            return self.value[index]\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ab45acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自\n",
      "33260\n",
      "然\n",
      "38380\n",
      "-2\n",
      "自\n",
      "33260\n",
      "然\n",
      "38380\n",
      "语\n",
      "74203\n",
      "言\n",
      "38383\n",
      "-4\n",
      "不\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "dic = {'自然':'nature','自然人':'human','自然语言':'language','自语':'talk to oneself','入门':'introduction'}\n",
    "dat = DoubleArrayTrie(dic)\n",
    "assert dat['自然'] == 'nature'\n",
    "assert dat['自然语言'] == 'language'\n",
    "assert dat['不自然'] is None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de79f8aa",
   "metadata": {},
   "source": [
    "## 4.3.AC自动机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94e60635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classic_demo():\n",
    "    words = [\"hers\", \"his\", \"she\", \"he\"]\n",
    "    Trie = JClass('com.hankcs.hanlp.algorithm.ahocorasick.trie.Trie')\n",
    "    trie = Trie()\n",
    "    for w in words:\n",
    "        trie.addKeyword(w)\n",
    "\n",
    "    for emit in trie.parseText(\"ushers\"):\n",
    "        print(\"[%d:%d]=%s\" % (emit.getStart(), emit.getEnd(), emit.getKeyword()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0050f41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2:3]=he\n",
      "[1:3]=she\n",
      "[2:5]=hers\n"
     ]
    }
   ],
   "source": [
    "classic_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2a6410",
   "metadata": {},
   "source": [
    "## 4.4.基于双数组字典树的AC自动机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1b9c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classic_demo():\n",
    "    words = [\"hers\", \"his\", \"she\", \"he\"]\n",
    "    map = JClass('java.util.TreeMap')()     # 创建TreeMap实例\n",
    "    for word in words:\n",
    "        map[word] = word.upper()            # 存放键值对\n",
    "    trie = JClass('com.hankcs.hanlp.collection.AhoCorasick.AhoCorasickDoubleArrayTrie')(map)\n",
    "    for hit in trie.parseText(\"ushers\"):    # 遍历查询结果\n",
    "        print(\"[%d:%d]=%s\" % (hit.begin, hit.end, hit.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37af3543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1:4]=SHE\n",
      "[2:4]=HE\n",
      "[2:6]=HERS\n"
     ]
    }
   ],
   "source": [
    "classic_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97cad098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[江西, 鄱阳湖, 干枯, ，, 中国, 最大, 淡水湖, 变成, 大草原]\n"
     ]
    }
   ],
   "source": [
    "HanLP.Config.ShowTermNature = False\n",
    "segment = JClass('com.hankcs.hanlp.seg.Other.AhoCorasickDoubleArrayTrieSegment')(HanLP.Config.CoreDictionaryPath)\n",
    "print(segment.seg(\"江西鄱阳湖干枯，中国最大淡水湖变成大草原\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f5c4db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[江西, 鄱阳湖, 干枯, ，, 中国, 最, 大, 淡水湖, 变成, 大, 草原]\n",
      "[上海市, 虹口区, 大连西路, 5, 5, 0, 号, S, I, S, U]\n",
      "[上海市/ns, 虹口区/ns, 大连西路/ns, 550/m, 号/q, SISU/nx]\n",
      "单词:上海市 词性:ns\n",
      "单词:虹口区 词性:ns\n",
      "单词:大连西路 词性:ns\n",
      "单词:550 词性:m\n",
      "单词:号 词性:q\n",
      "单词:SISU 词性:nx\n"
     ]
    }
   ],
   "source": [
    "from pyhanlp.static import HANLP_DATA_PATH\n",
    "\n",
    "HanLP.Config.ShowTermNature = False\n",
    "dict1 = HANLP_DATA_PATH + \"/dictionary/CoreNatureDictionary.mini.txt\"\n",
    "dict2 = HANLP_DATA_PATH + \"/dictionary/custom/上海地名.txt ns\"\n",
    "\n",
    "segment = DoubleArrayTrieSegment(dict1)\n",
    "print(segment.seg('江西鄱阳湖干枯，中国最大淡水湖变成大草原'))\n",
    "\n",
    "segment = DoubleArrayTrieSegment([dict1, dict2])\n",
    "print(segment.seg('上海市虹口区大连西路550号SISU'))\n",
    "\n",
    "segment.enablePartOfSpeechTagging(True)\n",
    "HanLP.Config.ShowTermNature = True\n",
    "print(segment.seg('上海市虹口区大连西路550号SISU'))\n",
    "\n",
    "for term in segment.seg('上海市虹口区大连西路550号SISU'):\n",
    "    print(\"单词:%s 词性:%s\" % (term.word, term.nature))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40967c7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tests'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyhanlp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtest_utility\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ensure_data\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tests'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyhanlp import *\n",
    "from tests.test_utility import ensure_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd50e224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
